{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from IPython.display import display \n",
    "from PIL import Image\n",
    "from keras.applications.resnet50 import ResNet50 \n",
    "import time\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"/mnt/disks/patches/calcifications/train/\"\n",
    "TEST_DIR = \"/mnt/disks/patches/calcifications/test/\"\n",
    "IM_WIDTH, IM_HEIGHT = 256, 256\n",
    "FC_SIZE = 256\n",
    "batch_size = 100\n",
    "NUM_CLASSES = 4\n",
    "NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    "  )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bea15df1bb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_device_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m model.add(ResNet50(include_top = False, \n\u001b[1;32m      6\u001b[0m                    \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ResNet50(include_top = False, \n",
    "                   weights=None, \n",
    "                   input_shape = (256, 256, 3), classes = NUM_CLASSES))\n",
    "\n",
    "## Add in last 3 layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "filepath=\"/home/jlandesman/model_history/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "## Parallelize to attempt multi-GPU effort\n",
    "parallel_model = multi_gpu_model(model, gpus=2)\n",
    "parallel_model.compile(optimizer= Adam(lr=0.0002, beta_1=0.9, beta_2=0.999), \n",
    "                                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer= Adam(lr=0.002, beta_1=0.9, beta_2=0.999), \n",
    "#                                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "modelFit = parallel_model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples/batch_size,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            verbose = 1,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=test_generator.samples/batch_size, \n",
    "            callbacks=callbacks_list)\n",
    "end = time.time()\n",
    "\n",
    "total_time = int(end-start)\n",
    "time_per_epoch = total_time/NUM_EPOCHS\n",
    "\n",
    "forecasted_time = 100000 * time_per_epoch / (train_generator.samples + test_generator.samples)\n",
    "print ()\n",
    "print ()\n",
    "print (\"Model took \" + str(total_time) + \" seconds to run\" )\n",
    "print (\"Model takes \" + str(time_per_epoch) + \" seconds to run\")\n",
    "print (\"Approximate time taken per epoch for 100,000 images is \" + str(forecasted_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_PATH = '/mnt/disks/patches/calcifications/train/no_tumor/'\n",
    "TEST_PATH =  '/mnt/disks/patches/calcifications/test/no_tumor/'\n",
    "\n",
    "TRAIN_DUMP_PATH = '/mnt/disks/patches/overflow_files/train/no_tumor'\n",
    "TEST_DUMP_PATH =  '/mnt/disks/patches/overflow_files/test/no_tumor'\n",
    "\n",
    "## Read in files\n",
    "no_tumor_train = os.listdir(TRAIN_PATH)\n",
    "no_tumor_test = os.listdir(TEST_PATH)\n",
    "\n",
    "no_tumor_train = np.asarray(no_tumor_train)\n",
    "no_tumor_test = np.asarray(no_tumor_test)\n",
    "\n",
    "np.random.shuffle(no_tumor_train)\n",
    "np.random.shuffle(no_tumor_test)\n",
    "\n",
    "print(len(no_tumor_train))\n",
    "print(len(no_tumor_test))\n",
    "\n",
    "for file in no_tumor_train[0:200000]:\n",
    "    current_path = os.path.join(TRAIN_PATH, file)\n",
    "    dump_path = os.path.join(TRAIN_DUMP_PATH, file)\n",
    "    os.rename(current_path, dump_path)\n",
    "\n",
    "for file in no_tumor_test[0:50000]:\n",
    "    current_path = os.path.join(TEST_PATH, file)\n",
    "    dump_path = os.path.join(TEST_DUMP_PATH, file)\n",
    "    os.rename(current_path, dump_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#benign = os.listdir(TRAIN_PATH + 'benign')\n",
    "#benign_no_callback = os.listdir(TRAIN_PATH+'benign_no_callback')\n",
    "#malignant = os.listdir(TRAIN_PATH + 'malignant')\n",
    "\n",
    "## Build DF\n",
    "#file_paths = no_tumor# + benign + benign_no_callback + malignant\n",
    "#labels = ['no_tumor'] * len(no_tumor)# + ['benign'] * len(benign) + ['benign_no_callback'] * len(benign_no_callback) + ['malignant'] * len(malignant)\n",
    "#assert len(file_paths) == len(labels)\n",
    "\n",
    "# df = pd.DataFrame({'file_paths': file_paths, 'labels': labels})\n",
    "\n",
    "# ## Split into train/test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df['file_paths'], df['labels'], test_size = 0.2, random_state = 142)\n",
    "\n",
    "# ## Run\n",
    "# counter = 0\n",
    "# for label, file_name in zip(Y_test, X_test):\n",
    "#     current_dir = os.path.join(TRAIN_PATH + label +'/'+ file_name)\n",
    "#     test_dir = os.path.join(TEST_PATH + label +'/'+ file_name)\n",
    "#     os.rename(current_dir, test_dir)\n",
    "#     counter += 1\n",
    "#     if counter%1000 == 0:\n",
    "#         print ('Files moved; ', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
