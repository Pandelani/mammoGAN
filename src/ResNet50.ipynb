{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from IPython.display import display \n",
    "from PIL import Image\n",
    "from keras.applications.resnet50 import ResNet50 \n",
    "import time\n",
    "from keras.utils.training_utils import multi_gpu_model\n",
    "from keras.optimizers import Adam\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class ModelCheckpoint(Callback):\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and\n",
    "    keys in `logs` (passed in `on_epoch_end`).\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        filepath: string, path to save the model file.\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {auto, min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved (`model.save_weights(filepath)`), else the full model\n",
    "            is saved (`model.save(filepath)`).\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
    "                 save_best_only=False, save_weights_only=False,\n",
    "                 mode='auto', period=1):\n",
    "        super(ModelCheckpoint, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        num_outputs = len(self.model.outputs)\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best model only with %s available, '\n",
    "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch, self.monitor, self.best,\n",
    "                                     current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            self.model.layers[-(num_outputs+1)].save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            self.model.layers[-(num_outputs+1)].save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s did not improve' %\n",
    "                                  (epoch, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving model to %s' % (epoch, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    self.model.layers[-(num_outputs+1)].save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    self.model.layers[-(num_outputs+1)].save(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"/mnt/disks/patches/calcifications/train/\"\n",
    "TEST_DIR = \"/mnt/disks/patches/calcifications/test/\"\n",
    "IM_WIDTH, IM_HEIGHT = 256, 256\n",
    "FC_SIZE = 256\n",
    "batch_size = 110\n",
    "NUM_CLASSES = 4\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    "  )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IM_WIDTH, IM_HEIGHT),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(ResNet50(include_top = False, \n",
    "                   weights=None, \n",
    "                   input_shape = (256, 256, 3), classes = NUM_CLASSES))\n",
    "\n",
    "## Add in last 3 layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "model.load_weights('/home/jlandesman/model_history/weights-improvement-03-0.52.hdf5')\n",
    "\n",
    "\n",
    "filepath=\"/home/jlandesman/model_history/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "## Parallelize to attempt multi-GPU effort\n",
    "parallel_model = multi_gpu_model(model, gpus=2)\n",
    "parallel_model.compile(optimizer= Adam(lr=0.0002,beta_1=0.9, beta_2=0.999), \n",
    "                                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer= Adam(lr=0.002, beta_1=0.9, beta_2=0.999), \n",
    "#                                       loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "modelFit = parallel_model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples/batch_size,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            verbose = 1,\n",
    "            validation_data=test_generator,\n",
    "            validation_steps=test_generator.samples/batch_size, \n",
    "            callbacks=callbacks_list)\n",
    "end = time.time()\n",
    "\n",
    "total_time = int(end-start)\n",
    "time_per_epoch = total_time/NUM_EPOCHS\n",
    "\n",
    "forecasted_time = 100000 * time_per_epoch / (train_generator.samples + test_generator.samples)\n",
    "print ()\n",
    "print ()\n",
    "print (\"Model took \" + str(total_time) + \" seconds to run\" )\n",
    "print (\"Model takes \" + str(time_per_epoch) + \" seconds to run\")f\n",
    "print (\"Approximate time taken per epoch for 100,000 images is \" + str(forecasted_time) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_PATH = '/mnt/disks/patches/calcifications/train/no_tumor/'\n",
    "TEST_PATH =  '/mnt/disks/patches/calcifications/test/no_tumor/'\n",
    "\n",
    "TRAIN_DUMP_PATH = '/mnt/disks/patches/overflow_files/train/no_tumor'\n",
    "TEST_DUMP_PATH =  '/mnt/disks/patches/overflow_files/test/no_tumor'\n",
    "\n",
    "## Read in files\n",
    "no_tumor_train = os.listdir(TRAIN_PATH)\n",
    "no_tumor_test = os.listdir(TEST_PATH)\n",
    "\n",
    "no_tumor_train = np.asarray(no_tumor_train)\n",
    "no_tumor_test = np.asarray(no_tumor_test)\n",
    "\n",
    "np.random.shuffle(no_tumor_train)\n",
    "np.random.shuffle(no_tumor_test)\n",
    "\n",
    "print(len(no_tumor_train))\n",
    "print(len(no_tumor_test))\n",
    "\n",
    "for file in no_tumor_train[0:200000]:\n",
    "    current_path = os.path.join(TRAIN_PATH, file)\n",
    "    dump_path = os.path.join(TRAIN_DUMP_PATH, file)\n",
    "    os.rename(current_path, dump_path)\n",
    "\n",
    "for file in no_tumor_test[0:50000]:\n",
    "    current_path = os.path.join(TEST_PATH, file)\n",
    "    dump_path = os.path.join(TEST_DUMP_PATH, file)\n",
    "    os.rename(current_path, dump_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#benign = os.listdir(TRAIN_PATH + 'benign')\n",
    "#benign_no_callback = os.listdir(TRAIN_PATH+'benign_no_callback')\n",
    "#malignant = os.listdir(TRAIN_PATH + 'malignant')\n",
    "\n",
    "## Build DF\n",
    "#file_paths = no_tumor# + benign + benign_no_callback + malignant\n",
    "#labels = ['no_tumor'] * len(no_tumor)# + ['benign'] * len(benign) + ['benign_no_callback'] * len(benign_no_callback) + ['malignant'] * len(malignant)\n",
    "#assert len(file_paths) == len(labels)\n",
    "\n",
    "# df = pd.DataFrame({'file_paths': file_paths, 'labels': labels})\n",
    "\n",
    "# ## Split into train/test\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df['file_paths'], df['labels'], test_size = 0.2, random_state = 142)\n",
    "\n",
    "# ## Run\n",
    "# counter = 0\n",
    "# for label, file_name in zip(Y_test, X_test):\n",
    "#     current_dir = os.path.join(TRAIN_PATH + label +'/'+ file_name)\n",
    "#     test_dir = os.path.join(TEST_PATH + label +'/'+ file_name)\n",
    "#     os.rename(current_dir, test_dir)\n",
    "#     counter += 1\n",
    "#     if counter%1000 == 0:\n",
    "#         print ('Files moved; ', counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
